package it.unipd.dstack.butterfly.config;

import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.kafka.common.serialization.StringSerializer;

import java.util.Properties;

import static io.confluent.kafka.serializers.KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG;

public class KafkaPropertiesFactory {

    /**
     * Provides the default property configuration for Apache Kafka.
     * See https://docs.confluent.io/current/installation/configuration/producer-configs.html
     *
     * @return
     */
    public static Properties defaultKafkaProducerPropertiesFactory() {
        Properties props = new Properties();

        // A list of URLs to use for establishing the initial connection to the cluster.
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                ConfigManager.getStringProperty("KAFKA_BOOTSTRAP_SERVERS_CONFIG"));

        // The producer will attempt to batch records together into fewer requests whenever multiple records are being
        // sent to the same partition. This helps performance on both the client and the server. This configuration
        // controls the default batch size in bytes.
        // No attempt will be made to batch records larger than this size.
        props.put(ProducerConfig.BATCH_SIZE_CONFIG,
                ConfigManager.getIntProperty("KAFKA_BATCH_SIZE_CONFIG"));

        // The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records
        // are sent faster than they can be delivered to the server the producer will block for max.block.ms after which
        // it will throw an exception.
        props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,
                ConfigManager.getIntProperty("KAFKA_BUFFER_MEMORY_CONFIG"));

        // An ID string to pass to the server when making requests. The purpose of this is to be able to track the
        // source of requests beyond just ip/port by allowing a logical application name to be included in server-side
        // request logging.
        props.put(ProducerConfig.CLIENT_ID_CONFIG,
                ConfigManager.getStringProperty("KAFKA_CLIENT_ID_CONFIG"));

        // The compression type for all data generated by the producer. Compression is of full batches of data, so the
        // efficacy of batching will also impact the compression ratio (more batching means better compression).
        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,
                ConfigManager.getStringProperty("KAFKA_COMPRESSION_TYPE_CONFIG", "lz4"));

        // The base amount of time to wait before attempting to reconnect to a given host.
        // This avoids repeatedly connecting to a host in a tight loop.
        // This backoff applies to all connection attempts by the client to a broker.
        props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG,
                ConfigManager.getIntProperty("KAFKA_RETRY_BACKOFF_MS_CONFIG", 1000));

        // The number of acknowledgments the producer requires the leader to have received before considering a request
        // complete. This controls the durability of records that are sent.
        props.put(ProducerConfig.ACKS_CONFIG,
                ConfigManager.getStringProperty("KAFKA_ACKS_CONFIG", "all"));

        // Setting a value greater than zero will cause the client to resend any record whose send fails with a
        // potentially transient error. Note that this retry is no different than if the client resent the record upon
        // receiving the error. Allowing retries without setting max.in.flight.requests.per.connection to 1 will
        // potentially change the ordering of records because if two batches are sent to a single partition, and the
        // first fails and is retried but the second succeeds, then the records in the second batch may appear first.
        props.put(ProducerConfig.RETRIES_CONFIG,
                ConfigManager.getIntProperty("KAFKA_RETRIES_CONFIG", 10));

        // The configuration controls how long KafkaProducer.send() and KafkaProducer.partitionsFor() will block.
        // These methods can be blocked either because the buffer is full or metadata unavailable.Blocking in the
        // user-supplied serializers or partitioner will not be counted against this timeout.
        props.put(ProducerConfig.MAX_BLOCK_MS_CONFIG,
                ConfigManager.getIntProperty("KAFKA_MAX_BLOCK_MS_CONFIG", 600000));

        // Serializer class for key that implements the org.apache.kafka.common.serialization.Serializer interface.
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,
                StringSerializer.class.getName());

        // Serializer class for value that implements the org.apache.kafka.common.serialization.Serializer interface.
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,
                KafkaAvroSerializer.class.getName());

        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,
                ConfigManager.getStringProperty("AVRO_SCHEMA_REGISTRY_URL"));

        return props;
    }

    public static Properties defaultKafkaConsumerPropertiesFactory() {
        Properties props = new Properties();

        // A list of URLs to use for establishing the initial connection to the cluster.
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,
                ConfigManager.getStringProperty("KAFKA_BOOTSTRAP_SERVERS_CONFIG"));

        props.put(ConsumerConfig.GROUP_ID_CONFIG,
                ConfigManager.getStringProperty("KAFKA_GROUP_ID_CONFIG"));

        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,
                ConfigManager.getStringProperty("KAFKA_AUTO_OFFSET_RESET_CONFIG", "earliest"));

        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,
                ConfigManager.getBooleanProperty("KAFKA_ENABLE_AUTO_COMMIT_CONFIG", false));

        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG,
                ConfigManager.getIntProperty("KAFKA_MAX_POLL_RECORDS_CONFIG", 10));

        // Deserializer class for key that implements the
        // org.apache.kafka.common.serialization.Deserializer interface.
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                StringDeserializer.class.getName());

        // Deserializer class for value that implements the
        // org.apache.kafka.common.serialization.Deserializer interface.
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
                KafkaAvroDeserializer.class.getName());

        props.put(SPECIFIC_AVRO_READER_CONFIG, "true");

        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG,
                ConfigManager.getStringProperty("AVRO_SCHEMA_REGISTRY_URL"));

        return props;
    }
}
